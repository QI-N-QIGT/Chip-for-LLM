# 1.2 主流 LLM 架构演进（Architecture Evolution of Mainstream LLMs）

本章以时间线与技术脉络为主线，系统梳理主流大语言模型（LLM）的演化进度、架构特点与关键创新，并结合“Chip for LLM”的视角讨论这些创新背后的工程动机、硬件影响与系统协同。力求以叙述性和体系化的方式，呈现从早期 Transformer 到当代多样化范式（稀疏 MoE、长上下文、模态扩展、状态空间模型等）的发展图景。

---

## 一、演化时间线与阶段性特征

- 2018–2019：从通用 Transformer 到预训练范式奠基
  - 代表模型：BERT（Encoder-only）、GPT-1/2（Decoder-only）、Transformer-XL（相对位置与长依赖）。
  - 核心转变：从监督学习转向大规模自监督预训练；自注意力成为通用序列建模的默认范式；预训练 + 微调模式确立。
  - 工程要点：仍以绝对/相对位置编码、后归一化（Post-LN）为主；模型规模较小，内存/带宽压力相对可控。

- 2020–2021：大参数规模化与稀疏激活的首次兴起
  - 代表模型：GPT‑3（175B）、T5（Encoder‑Decoder）、Switch Transformer（MoE）。
  - 核心转变：参数规模突增，训练数据与计算预算同步增长；MoE 引入“条件计算”，以固定计算预算实现更大有效容量。
  - 工程要点：训练侧的并行化（数据/张量/流水/专家并行）与内存优化（混合精度、检查点）走向体系化；推理侧 KV Cache 初显压力。

- 2022：数据与计算的“等价交换”与训练效率主义
  - 代表模型/理念：Chinchilla（“以数据换参数”的缩放规律）、PaLM 系列、大规模开源复现（OPT/BLOOM 等）。
  - 核心转变：强调在给定计算预算下，较小参数但更多数据的训练策略更优；训练“计算最优”成为共识之一。
  - 工程要点：BF16/FP16 混合精度成熟；优化器状态与激活检查点显著降低显存占用；IO 与带宽瓶颈愈发突出。

- 2023：高质量开源与体系化优化的“收敛”
  - 代表模型：LLaMA/LLaMA‑2、Mistral、Mixtral（稀疏 MoE）、多家开源家族（Qwen、Phi 等）。
  - 核心转变：
    - 架构上趋于收敛：Pre‑LN、RMSNorm、SwiGLU、RoPE、GQA/MQA 等成为“实用主义”的组合基线。
    - 稀疏 MoE 进入实用阶段：Top‑2 路由、负载均衡损失、容量因子控制稳定性与吞吐/延迟权衡。
    - 长上下文能力成为关键竞争点：基于 RoPE 插值、ALiBi、位置扩展/插值与滑动窗口注意力等方法。
  - 工程要点：推理系统（如分块/分页 KV、注意力核融合、页式调度）与硬件（HBM、NVLink/IB）协同显著提升端到端性能。

- 2024 至今：多模态、长上下文与新范式并进
  - 代表方向：多模态 LLM（LMM）、SSM（如 Mamba）与混合架构（Transformer×SSM）、指令/工具增强与智能体式推理等。
  - 核心转变：不仅比拼参数量，更注重数据质量、任务泛化、上下文长度、工具使用、知识检索与跨模态理解。
  - 工程要点：更极致的内存压缩（KV 量化/稀疏）、更高效的注意力核（IO‑aware）、更稳健的专家通信与路由并行。

---

## 二、主流架构基线与模块演进

本节总结当前高性能 LLM 常用的“实战基线”，及其背后的工程动机与硬件影响。

1) 编码范式：Decoder‑Only 为主，Encoder‑Decoder 在特定场景保留
- Decoder‑Only（GPT/LLaMA/Mistral 等）
  - 特点：自回归生成、推理时 KV Cache 成本与延迟为核心矛盾。
  - 适用：通用生成、对话与指令遵循、多任务扩展。
- Encoder‑Decoder（T5/BART 等）
  - 特点：编码器捕获上下文，解码器生成；适合 seq2seq（翻译、摘要）。
  - 影响：在多模态或检索增强时，结构更自然但系统与资源开销更复杂。

2) 位置表示：从绝对到相对，再到可扩展的旋转/线性偏置
- 绝对位置编码：简单但对长上下文泛化差。
- 相对位置编码/偏置：更好地建模距离关系与迁移。
- RoPE（旋转位置编码）：以复平面旋转注入相位信息，便于长上下文插值与扩展；硬件友好（张量形状保持）。
- ALiBi（线性偏置）：以距离相关的线性偏置替代显式位置嵌入，扩展更平滑，额外存储开销小。

3) 归一化与残差布置：Pre‑LN 与 RMSNorm 成为主流
- Pre‑LN：改善深层训练稳定性，收敛更稳；
- RMSNorm：去掉均值归一化，数值稳定性好，计算更简洁；
- 硬件影响：更利于低精度训练/推理（FP8/INT8）稳定。

4) 前馈网络（FFN）：Gated 激活成为“性价比”选择
- GELU → SwiGLU/Gated‑MLP：在参数增量不大前提下提升表示能力与收敛速度；
- 工程影响：FFN 占据大量 GEMM 工作量，门控结构对算子融合与张量形状敏感，需优化 kernel 以减少内存访存与重排。

5) 注意力与 KV Cache：从全量多头到 MQA/GQA 与长上下文技巧
- 多头注意力 → MQA（Multi‑Query）/GQA（Grouped‑Query）
  - 动机：降低 KV Cache 存储/带宽与推理延迟；
  - 折中：少量共享键值头可能带来轻微质量损失，需要在 head 维度/组大小上权衡。
- 长上下文增强
  - RoPE 插值/缩放、位置插值、ALiBi、滑动窗口注意力（SW‑Attention）、注意力 Sink/锚点等；
  - 系统侧：Paged/Chunked KV、KV 量化（INT4/NF4/FP8）、KV 淘汰/压缩策略（滑窗保近、稀疏保远）。
- IO 感知注意力内核
  - 通过块化/重排最小化 HBM 访存，显著提升吞吐与可扩展性；对芯片侧缓存层级与片上 SRAM 的利用提出更高要求。

6) Tokenizer 与词表：字节级与多语言覆盖
- 趋势：字节级/子词级（BPE/SentencePiece）词表，兼容多语言与代码字符集；
- 影响：更稳定的跨域泛化与更低的 OOV；对 embedding/Gather 的随机访问模式提出缓存友好性挑战。

7) 训练与并行：计算最优与内存/通信最优的三难平衡
- 训练策略：Chinchilla 式“数据优先”、混合精度（BF16/FP8）、梯度检查点、优化器状态分片；
- 并行维度：数据并行、张量并行、流水并行、专家并行（MoE）；
- 通信与内存：参数与激活的分布式存取、All‑Reduce/All‑to‑All 的网络瓶颈、NVLink/IB 的带宽利用、拓扑感知放置与调度。

---

## 三、独特架构创新与所解决的问题

1) 稀疏 MoE（Mixture‑of‑Experts）：以条件计算获取“更大有效容量”
- 核心思想：在每个 token 上只激活少数专家（如 Top‑2），以近似固定的计算成本换取更大的参数容量与表示多样性。
- 关键组件：路由器（gating）、专家 FFN、负载均衡损失、容量因子（Capacity Factor）、温度/噪声稳定化。
- 训练难点：
  - 路由不均衡导致专家过载/闲置，需要正则与容量控制；
  - All‑to‑All 通信成为主要瓶颈，需专家并行和拓扑感知调度；
  - 稳定性问题（drop token、抖动）需在损失与实现层面协同优化。
- 推理难点：
  - Token‑Expert 动态分发带来延迟抖动与通信开销；
  - 工程上通过局部专家（Local MoE）、批内重排、路由缓存、跨设备专家复用等方式降低尾延迟。
- 解决问题：在计算预算基本不变的前提下提升质量与任务覆盖；在同等质量下显著降低计算成本。
- 硬件/系统影响：All‑to‑All 对互联带宽/延迟极为敏感；专家放置策略、通信压缩与流水并行成为系统优化关键。

2) 长上下文建模：从位置扩展到记忆机制
- 工程痛点：自回归解码的 KV Cache 随上下文线性增长，带来容量与带宽双重压力，且长距离注意力退化。
- 架构方法：
  - 位置扩展：RoPE/ALiBi/插值与缩放策略，尽量保持原训练分布的外推稳定性；
  - 稀疏/局部注意力：滑动窗口、块稀疏、跨块金字塔；
  - 记忆/检索增强：RAG/Memory Token/外部向量库，降低模型“死记硬背”的负担。
- 系统方法：
  - KV 的分页化/块化管理，跨请求复用与按需换入；
  - KV 低比特量化与近似压缩，在可接受失真下显著降低带宽；
  - 流式/增量计算与核融合减少 IO。
- 解决问题：在可控资源消耗下，将上下文长度扩展到数万甚至更长，提高长文理解、一致性与工具调用的上下文可见性。

3) GQA/MQA：面向推理效率与 KV 可扩展性的注意力变体
- 动机：多头注意力的 KV 缓存线性增长导致显存/带宽压力过大。
- 做法：让多个查询头共享更少的键值头（MQA）或按组共享（GQA），减少 KV 容量与读写次数。
- 折中：轻微的表达能力下降需通过更多训练数据、微调或更优参数化弥补；组大小与头维度的工程权衡至关重要。
- 效果：显著提升小批量低延迟推理的吞吐与并发，降低服务成本。

4) IO‑Aware 的注意力核与算子融合：用“数据移动最少”换“速度最大”
- 背景：现代加速器的能耗与瓶颈常由内存访问而非计算本身主导。
- 方法：块化/重排/流水执行，最大化片上复用，最小化 HBM 往返；将 QK^T、Softmax、V 投影与后续归并在内核级融合。
- 影响：在不改变模型函数形式的前提下，解锁大幅吞吐提升与长序列可扩展性；推动硬件侧更大片上缓存与更高带宽设计。

5) 多模态对齐与轻量化视觉/音频前端：向“通用输入”扩展
- 模块化做法：采用预训练的视觉/音频编码器，经线性/MLP 投影到 LLM 词嵌入空间，通过跨注意力或拼接实现模态对齐。
- 工程挑战：视觉 Token 体量大、时序 Token 冗长；引入 Patch 合并、稀疏采样与关键信息保留策略。
- 解决问题：降低模态融合成本，让通用 LLM 具备理解图像、语音与视频的能力，拓展应用边界。

6) 新范式探索：SSM（状态空间模型）与混合架构
- 动机：Transformer 的二次复杂度在超长序列上不可持续；
- 方法：以状态空间递推替代全局注意力，理论上实现线性复杂度与更好的硬件亲和性；
- 现状：在特定任务/长度区间展现潜力，亦出现 Transformer×SSM 的混合方案以兼顾局部与全局依赖。

---

## 四、代表模型系谱与架构画像（精选）

- BERT（Encoder‑Only）
  - 画像：双向掩码建模（MLM），适用于理解类任务（分类、抽取）。
  - 贡献：奠定预训练 + 下游微调范式；为后续 Encoder‑Decoder 类模型提供基础。
  - 局限：生成能力弱，难以胜任开放式对话、创作与工具调用。

- T5（Encoder‑Decoder）
  - 画像：统一“文本到文本”范式，强于结构化转换（翻译、摘要、问答）。
  - 贡献：通过任务前缀与统一接口，实现多任务学习与迁移。
  - 工程：训练与推理链路更复杂，系统开销较 Decoder‑Only 更高。

- GPT 系列（Decoder‑Only）
  - 画像：自回归大模型的标杆；指令对齐后具备强泛化与工具使用潜力。
  - 贡献：推动大规模生成式预训练，形成通用对话与推理框架。
  - 工程：对解码延迟极为敏感，驱动 MQA/GQA、KV 压缩、推理系统优化与并行采样等创新。

- LLaMA 家族（Decoder‑Only，开源生态）
  - 画像：以 RoPE、Pre‑LN、RMSNorm、SwiGLU、GQA 等组成实践友好的基线；强调数据质量与指令对齐。
  - 贡献：树立高质量开源基线，带动社区在长上下文、量化、蒸馏与工具链上的快速迭代。
  - 工程：在 7B–70B 量级覆盖推理到训练多场景，适合复现与二次开发。

- Mistral/Mixtral（高效基线与稀疏 MoE）
  - 画像：Mistral 采用滑动窗口注意力与 GQA，强调高效与长上下文兼容；Mixtral 以 Top‑2 稀疏 MoE 实现“同算力更高质量”。
  - 贡献：将 MoE 推向实用，并在开源体系内实现稳定训练与推理；
  - 工程：专家并行与通信调度成为端到端性能的决定因素。

- Qwen、Phi 等开源家族（数据与工程并重）
  - 画像：在中小参数规模下，通过高质量数据与工程优化取得强性能/成本比；
  - 贡献：推动“小而强”的模型路线，利于边缘与专用场景落地；
  - 工程：更依赖 tokenizer 设计、数据蒸馏与高效推理（量化/稀疏）。

- 多模态 LLM（LMM）
  - 画像：将视觉/音频等模态对齐到语言空间，实现感知 + 语言的融合；
  - 贡献：拓展 LLM 的输入边界，支持看图对话、视频理解与听写理解；
  - 工程：视觉/音频 token 的压缩与跨模态注意力的访存成为关键优化点。

- SSM 与混合模型（如 Mamba、Transformer×SSM）
  - 画像：以线性时间递推替代二次注意力或与之互补；
  - 贡献：在超长序列与流式场景中展现潜在优势；
  - 工程：需要全新 kernel 与编译优化，与 GPU 张量核心的耦合度仍在演进中。

---

## 五、从芯片与系统视角的影响与协同

- 计算特征：GEMM 仍是“绝对主角”，但性能瓶颈常在 HBM 带宽与片上缓存利用。
  - 桥接策略：算子融合、张量布局优化、流水并行、持久化缓存；
  - 低精度：FP8/INT8 在训练/推理两侧推进，需配合量化感知训练与校准。

- 内存与缓存：KV Cache、激活与优化器状态三类核心占用
  - 推理侧：KV 的线性增长是延迟与成本的主要来源；MQA/GQA、KV 量化/分页是现实可行路径；
  - 训练侧：激活重计算、ZeRO/分片策略、检查点化平衡显存与速度。

- 通信与互联：All‑Reduce 与 All‑to‑All 主导集群可扩展性
  - 张量并行/流水并行注重 All‑Reduce 的拓扑与频率；MoE 强依赖 All‑to‑All；
  - NVLink/PCIe/IB/CXL 的带宽与延迟、拓扑匹配与亲和调度决定集群效率上限。

- 推理系统：从“核更快”到“端到端延迟更小”
  - 分块/分页 KV、批内/跨请求并行、动态路由与合并、流控与抢占；
  - 推测解码、并行采样、树状/多样性解码等策略需与缓存与带宽策略协同设计。

- 量化与稀疏：从“单算子可行”到“系统级收益”
  - 激活/权重/KV 的协同量化与误差控制；
  - 稀疏化（非结构化/半结构化/块稀疏）与硬件原生支持之间的平衡。

---

## 六、趋势展望与实践建议

- 趋势一：数据质量与训练策略的“计算最优”继续主导，参数规模不再单一追求。
- 趋势二：长上下文将走向“架构 + 系统”一体化（位置扩展 × KV 管理 × 量化稀疏）。
- 趋势三：稀疏 MoE 的产业化，围绕专家并行、通信压缩与鲁棒路由的系统共优化将成为关键。
- 趋势四：多模态轻量前端与跨模态压缩技术，成为 LLM 走向通用感知的必经之路。
- 趋势五：新范式（SSM/混合架构）与新硬件（更大片上缓存、更高带宽/低延迟互联、近内存计算）将互相牵引。

实践建议（针对“Chip for LLM”的落地方向）：
- 基线优先：以 RoPE + Pre‑LN + RMSNorm + SwiGLU + GQA/MQA 的 Decoder‑Only 为通用基线；
- 长上下文：结合 RoPE 插值 + 分页/量化 KV + 滑动/稀疏注意力；
- 推理系统：采用 IO‑aware 注意力核与算子融合，重视批内并行与缓存复用；
- MoE 场景：优先局部专家与拓扑感知放置，关注 All‑to‑All 的拥塞控制与路由稳定；
- 多模态：独立预训练编码器 + 轻量对齐头，控制视觉/音频 token 体量；
- 硬件共设：为 KV 与激活的片上复用预留空间，评估 FP8/INT8 的数值稳定方案与量化误差控制。

---

通过上述梳理可以看到，主流 LLM 的架构演进正从“单点创新”转向“架构—系统—硬件”的协同优化：以更可训练、可部署、可扩展为目标，围绕 KV Cache、互联与带宽、低精度与稀疏、长上下文与多模态等关键矛盾持续演进。这一趋势将长期影响芯片架构、系统软件与模型设计三者的协同边界。
